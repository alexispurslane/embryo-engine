### Why this engine?

Two of the most interesting and under-served styles of games in the indie game development landscape are immersive sims and Bethesda-style open world RPGs. (In fact, I don't think *anyone* tries to make games like the ones Bethesda makes, but that's a whole different story.) As someone who is deeply interested in both those genres, this seems like a shame to me.

Of course, there are obvious reasons for their rarity --- namely, that both genres require a ton of work, with large and complex levels or worlds, tons of missions/quests, lots of game mechanics, and complex interactions to deal with, requiring immense amount of programming, game design, and artistic effort. I can't help with those problems directly, although I think they can be mitigated by just purposefully taking a smaller focus and managing player expectations to match. 

However, there are reasons I *can* help with too: namely, that the existing mainstream game engines available to your average game developer just aren't very well-suited to the task of making these styles of games, at a basic engine architecture and optimizations level, and more interestingly perhaps, that mainstream engines aren't really optimized for enabling development of games in a dynamic, componentized, moddable, open, version-controlled fashion, without specialized tools and formats that lock you into one specific ecosystem, which would enable games to more easily be developed by larger indie teams, OSS communities, or develop large modding communities.

This is what motivated me to start writing this engine: I want to write a game engine that captures as many of the unique technical decisions and features of the engines that make my favorite games possible as I can, and combine them with modern game engine architecture methodology and a focus on scriptability, moddability, and easy to modify open formats, to create the best engine for this sort of thing I can imagine.

So, what does that look like?

Well, one of the things that makes Bethesda's games truly special is their special fork of Gamebryo, eventually called the Creation Engine. Despite the bad rap it gets[^15] and the detriments it may have, it contributes significantly to the unique gameplay character and feel of Bethesda games, with their unusually simulation-heavy worlds, with roaming NPCs that are all as fully-simulated as the player character itself, with their own goals and routines, where every item you have in your inventory and decoration you find around the map, is a real, simulated physics object you can pick up and play with, and where RPG-like statistics and physics are translated into visible, tangible FPS or third-person gameplay mechanics, all set in gigantic, seamless open worlds, and their composable, moddable nature, where even core game content is implemented in exactly the same way any average player's mods could be, meaning that player mods are empowered to interact deeply with the game and its mechanics, making significant changes or adding significant content, and meaning that adding to the game mechanics and expanding game content is far easier than on almost any other engine. I think this is made possible by an extremely data-oriented design, where creating new game entities and maps is as simple as filling out database forms, and creating new kinds is similarly easy, and where different properties can be easily added or removed from game entities whenever you want, while deeply modifying the game behavior is as easy as dropping a compiled script into a folder. Therefore, if I want a game engine that enables games that *feel like Bethesda games*, I'll want to make a game engine that is similarly, or even more, focused on making modding the primary game development workflow, making it as dynamic and composible as possible, and ensuring that as much of the game can be trivially mixed and matched, and created with simple data entry.

The lesson to learn from immersive sims, on the other hand, are more focused around design requirements, not specific technical aspects, but one does directly lead to another. To enable immersive sim style gameplay as much as possible, game object behavior and properties need to be extremely modular and encapsulated, to avoid overwhelming complexity, dynamically composable, to allow for great runtime freedom, and capable of influencing other properties and especially behavior *without explicit coupling*, so unforeseen combinations can occur. Luckily, there are systems used in the *System Shock*, *Thief*, and *Caves of Qud* games that can help us with this. For more see below.

## Deeper exploration of design principles

### Data-driven design

The most unique thing about the Gamebryo engine is that it has an extremely data-driven[^1] design.

Essentially, this means that all of the game-specific logic and content, from the game world to the game objects, NPCs, quest systems, game win and loss states, game mechanics, actor behavior, and more, are specified using plain old data and a simple scripting language, which the game engine then picks up and runs much in the same way VLC plays a movie, instead of much of the game-specific content having to be written in the game engine's language itself and statically linked to the engine or compiled into the game engine's executable to produce the final game. This data driven design is even reflected in the final game output: if you look at the install folder of any Bethesda game, you will see an engine executable, but you'll also see that all of the game's scripts and content are separate from that executable. Not plain text anymore, mind you, it's all been compiled and compressed, but still definitely separate, which is why you can easily add *new* scripts for the engine to load! This is a difference of degree, not kind, from modern general purpose engines like Unreal and Unity, and even some modern big budget studio engines like REDEngine, but it is a very large degree of difference nonetheless. There is a reason Bethesda's own first-party DLC content for their games takes the exact same form that large community content mods do, which cannot be said for any other engine.

This data-driven design has several benefits. First, it makes adding content to much less time consuming, simpler, and easier to iterate on, making large, content-rich, interconnected worlds more feasible. Second, by making a system by which one can easily and declaratively add new objects and even mechanics to the game, a scripting system with total access to the full power of the engine's capabilities --- since that's the main way games will be programmed --- and a structure that allows adding new scripts and removing old ones even after the game has been "compiled," this data-driven architecture enables a much more vibrant and empowered modding community, since first-party content and mods are essentially on a level playing field: they're both doing the same thing, meaning mods have the full power game developers also have access to.

A highly data-driven architecture also has the further (theoretical) benefit that if sufficient compatibility is maintained between the data formats old and new versions of the engine accept, old games can be upgraded with new versions of the engine at any time with minimal need to actually edit the game!

The Embryo game engine aims to take this much, *much* further, by ensuring that all the data files used by the engine are in readily readable/writable open standards, instead of odd proprietary or in-house formats, and that even more of the game-specific content and behavior can be separated out from the engine, instead of needing a custom build of the engine or needing to be compiled into the engine for the final build.

In essence, Embryo Engine wants to be the Emacs of the game engine world. Just like Emacs is a relatively small core of optimized C code that mostly acts as a specialized yet powerful interface and rendering system and interpreter for a highly specialized scripting language, and most of the actual editor has been made with that scripting language, Embryo wants to act as an optimized, specialized, but extremely powerful core, on top of which most content will be built using its scripting language. Likewise, that language should have full access to the entirety of the engine, to insert itself wherever it pleases and access all of the engine's capabilities and power, instead of having a limited set of interface APIs for predefined tasks that the game developers figured mods should be able to do, just like Emacs Lisp is a far more powerful plugin system than the limited scripting API provided by NeoVim.

Here are the formats that Embryo will probably use:

- for configuration files, initial game world specifications, game object   (including actor) specifications, materials, and more, TOML files will be used - for compiled game world files, game world chunks, and save files, MessagePack   will be used - for textures, simple image formats will be used - for heightmaps, BMPs will be used (in a specific way) - for 3d objects and parts of scenes, glTF 2.0 will be used
- for scripting, **WebAssembly** will be used --- that is, you'll hopefully be able to use any language that can be compiled to WebAssembly to script your game, and you'll be able to mod someone else's game with a totally different language than the one they used --- perhaps so you can use a language you're more comfortable with --- as long as your new language also compiles to WebAssembly! This should allow an incredibly diverse, thriving polyglot ecosystem. Want to use C#? Done. TypeScript? Excellent! Lisp? Sure. Haskell, OCaml? Why the hell not! Rust? Have it your way! In this sense, Embryo takes the scriptability level far beyond what even Emacs can offer --- besides offering a powerful scripting language that can be either interpreted, JIT'd, or even AOT compiled, it can be almost any scripting language *you want*!

This way, anyone will be easily able to create or modify game files without any specific suite of tools.

### Simulation-heavy and object-flexible

One of the other interesting aspects of Gamebryo is the fact that almost everything that exists in the game is simulated, to a much greater degree than in other game engines. For instance, NPCs are fully as detailed as players. Inventory items all have meshes and rigidbodies, instead of being ethereal power-ups, so they can act like real objects in the world. Many things in scenes can be picked up and moved around arbitrarily at will. All actors have rag-doll skeletons as well as animated ones. And so on. This is part of what gives Bethesda's games their unique flavor: unlike many other games of similar size and scope, the way you interact with the world is not limited to a few specific things you can do, outside of which the game will revert to an inert and lifeless rock despite all your tugging and prodding. In a sense, Bethesda games are much more of a simulated sandbox than other titles.

Of course, there is a drawback to this --- there are ***good reasons*** other studios don't follow in Bethesda's footsteps. Namely, that as you increase the simulational aspect of your game, you lose direct control over how the game behaves, opening yourself up to many more bugs and restricting your ability to tightly script, pace, and act things in your games. This is why while other games like Cyberpunk 2077 have breathtaking in-game cutscenes, even Bethesda's latest and least buggy game, Starfield, struggles to animate its characters through an emotional scene without the physics engine getting in the way.

Nevertheless, there is a crucial spot in the gaming world for such sandbox style games. In fact, there's a whole genre built around the idea that everything in the game world should be simulated and responsive to any reasonable thing you might want to try: immersive sims. From *System Shock 2* to *Deus Ex* to *Thief*, the key appeal of immersive sims is that you are given a large amount of powerful tools, and set loose on a problem in an interesting, endlessly responsive environment, to solve it however you like.

It may not seem obvious how to create a game engine that enables and accelerates the development of such simulation-heavy games, besides perhaps mandating that every game object have a mesh and a rigid body (a bad idea), but in watching gameplay from Deus Ex and Pray, as well as simulation-focused roguelikes like Dwarf Fortress and Caves of Qud[^2], a few things become clear. First of all, you need to be able to dynamically add and remove properties and behaviors from objects in combinations and at times that are not predictable ahead of time, and that to manage the complexity of such an endeavor, behavior and properties must be decoupled, encapsulated, and composable; second of all, that those behaviors, as well as individual game objects and actors, need to be able to effect each other's behavior without prior expectation of being able to do so; and finally, that specifying these packages of behavior and properties be as composible and declarative as possible. Let's look at each of these in turn.

- The first point could be simplistically enabled by just using a single game   object class for everything in the entire game, so that it contains all the   possible properties and behaviors any game object in the game could possibly   possess, such that producing any given behavior or combination thereof could   be produced in an object by just toggling on or off previously dormant   properties. The problem with this is, of course, that it introduces a lot of   coupling, problematic amounts of state management, and the possibility of   undesired interactions between properties, as well as wasting a lot of memory   and probably being difficult to maintain. An easier way is to use a simple   entity component system. This way, entities are just columns in a big table,   and components can be easily and dynamically added and removed from entities   as needed, where components represent compsible and encapsulated units of game   object properties and behavior is desired (and behavior itself is separated).

- One of the problems with the design of a classic entity component system, however, is that since all object behavior is defined in terms of systems,   which loop through all the entities with the necessary properties to have a   behavior and perform that behavior for each entity, communication between   behaviors or systems is difficult, and all combinatorial behavior must be   specified up front: if I want a new behavior to emerge when an entity has two   components at the same time, I have to either program that behavior into one   of the existing systems for those components, or create a new system that   operates only on entities with both components. Thus I must be able to predict   and architect all possible combinations of system behavior. This is especially   true as a result of the fact that it is difficult for systems to pass   per-entity temporary information --- events and messages --- to other systems   in an architecturally clean and encapsulated fashion, and difficult for them   to manipulate the behavior of other systems from afar with those messages   without tight coupling between them, because systems work on the basis of   regular behaviors, not event-based ones. Many architectural questions pop up   when trying to figure this out. Which system handles interlocking behavior?   How does one system modify the information another acts on, without modifying   the entity itself? Does the system generating the information modify the   entities the information is directed at? That produces tight coupling and   requires more foreknowledge about possible combinations. Furthermore, with a   classic system-oriented structure, parallel processing becomes more difficult:   if you have a series of systems that need to run on a group of entities in   order, you have to run the first system on all the entities, then the next,   and so on. However, the system might finish the step for some of those   entities sooner than others, in which case it would be desirable for those   entities which finished early to be able to move on from that step and work on   other steps while the late ones are on the last one. This is impossible in a   classic ECS system, which essentially requires a scatter/gather structure with   a sync point between each behavior, with at best a few unrelated systems   running at the same time. This is precisely the sort of design that Unreal had   that causes it to struggle to make use of modern multicore CPUs. Likewise,   passing information back and forth between threads in a classic ECS structure   is difficult. Thus I borrowed a concept from Erlang: message-passing and   Actor-oriented programming[^3], and a system from Naughty Dog called   fibers[^4]. In this model, there is a pool of operating system threads, one   per core, and jobs (or "fibers") that thread through one complete pipeline   task that needs to be performed in order are generated for any processing that   needs to be done that can be done in parallel, including actor behavior, and   pushed onto a job stack which the threads then pull from whenever they're   finished any previous jobs and begin working on. Specifically, in this case,   all the behavioral processing necessary for each game object and actor is   represented as a linear pipeline of transformations to the entity which can be   performed independently of any of the other entities, just based on last   frame's game state (as this process is producing this next frame's game   state). Thus, each actor can update as fast as it can, proceeding through the   pipeline. (This is actually related to how modern programmable pipelined GPUs   work.) When they need to communicate, they send messages to either a global   queue by message type, which each actor can subscribe to (such global messages   are called "events") or they send that message to an actor's unique queue.   General events in the world, such as collisons, entities entering trigger   areas, update ticks, user input, will be distributed to all actors via this   messaging and event system as well, meaning that *all actor behavior is   triggered by events/messages*, including stuff that happens on each frame. How   this works is inspired by the architecture of Caves of Qud's message based   ECS, where events are fed to the first behavior on an object (according to a   priority system), and that behavior can choose to act on any, all, or none of   those events, and then *modify those events* or produce new ones (either to   pass on, or to send to other entities, or to notify the game state that   something at a higher level needs to change), and this new set of events is   then passed to the next behavior. Thus, behaviors can modify each other by   modifying the events they receive, since all behavior is described by and   triggered by events, without any coupling whatsoever. I highly recommend you   watch the talk in the footnotes if you want to learn more about this.    - As for our third and final point, easy game object and behavior/property   specification and assembly in a declarative manner, that is already covered by   having a data-driven design.

### Knowing where to draw the line

Another principle of design I want to discuss here is less derived from immersive sims and the Gamebryo engine, and more one derived from necessity: in an attempt to limit the scope and scale of this endeavor to something relatively more reasonable, at least at the outset, I've set a specific era of games in mind that I want my engine's graphical, animation, and similar capabilities to be able to match, and beyond that, I'm not going to worry about it, besides making the engine extensible so that it's a good platform for doing more advanced things if people want to. Everything is a tradeoff between benefit and complexity, and for my limitations as an individual programmer, I've found that the graphics algorithms and similar capabilities of games from around 1998-2005 or so seem to be at the sweet spot of that tradeoff for me: any increase in capability increases complexity at a vastly disproportional rate compared to the actual tangible gains received in the meaningful artistic expressiveness of the game engine (not just shininess), especially given that the sheer power of modern hardware removes the real biggest limitations on the scope and expressiveness of games of those eras, whereas any decrease in capability diminishes the complexity of this endeavor only slightly, while walling off large portions of game expressiveness --- the types of games and visuals that can be made with the engine's technology.

Of course, I'm not blindly aiming for one level of technology. I'm using modern software architecture, data structures, and algorithms to make the engine performant and flexible and capable of taking advantage of modern hardware. I'm using modern data-oriented design, modern graphics programming techniques and APIs, and modern object model and parallel processing models. Thus, although it may look old on the surface, this engine will have fresh, shiny, and well thought through, modern internals. In effect I'm doing what pixel art indie devs have been doing for quite awhile now, keeping the scope and difficulty of developing their game's graphics and content limited by picking an old style of game to make with modern tools, so you get the productivity of modern tools brought to bear on simpler problems --- I'm just doing that for the polygon era, not the pixelated era!

Of course, this means it probably isn't for everybody, or even anybody, but, well. I'm making the program ;)

### Knowing specifically what you want and optimizing ruthlessly for that

In my opinion, carefully choosing data structures, algorithms, and overall architectures that are best suited to the specific use cases you have for them, and are acceptable in the general case, yet are as intuitive and straightforward as possible, will always win out over choosing the latest and greatest cutting edge algorithm from the newest research paper that supposedly has the best possible general case performance for every single thing, and trying to deal with the resultant complexity and unintuitiveness. This is because algorithms that are good in one specific case and fine in the general case are always going to be better in that specific case then algorithms that are trying to be good in all cases, and they are also always going to be easier to maintain because they are simpler and usually more intuitive, so if you know what kind of game you are going to make ahead of time, then you can outpace people doing more advanced things while also keeping your code simpler. It's a matter of slow and steady wins the race, in all honesty. I have a few small case studies in this that I will use to demonstrate how I apply this philosophy to my game engine.

First is my choice to use sparse set ECS instead of archetype based ECS[^7]. If you look at almost any major entity component system framework around today, such as flecs, Bevy, or Amethyst, you'll quickly notice that they are all using the exact same model for storing the components in their entity component system architectures: archetypes. Archetypes are typically used because they combine all entities with the same component signatures (and their associated components) in one place, so the systems that iterate over certain types of entities will be able to more quickly iterate through those entities because entities will be already grouped by type and have all of their components packed into their spot in that group already. This is an intuitive way to attempt to maximize the *data oriented design based* benefits of ECS --- since everything is done through batch processing via systems, you ensure that your systems can execute as fast as possible in three ways:

1. because only entities with a valid set of components for your system's query    or ever even iterated on in the first place, so there are no wasted    iterations where you have to discard a nonmatching entity,
2. similarly to above, because there isn't *branching* based on whether an    entity is a valid candidate for that system to process, for the same reason    as above, and 3. because the amount of the data that your system is processing that can fit    into your processor's cache at once is maximized, because all of the data is    packed as tightly as possible.

In essence, archetype-based ECS is primarily using entity component systems as a data structure that is used to do data oriented design style optimizations, and it is attempting to optimize for the general case --- the most common types of things you do with entity components systems.

So, why did I choose to go with a sparse set ECS instead? For two reasons:

1. if implemented correctly, sparse sets are not significantly slower than    archetypes for iteration or selecting entities with various components, and    thanks to EnTT-style groups there are in fact optimizations that you can do    for sparse ECS systems that can bring them up to parity or even superiority    to archetype-based ECS in that area, if needed, but compared to    archetype-based ECS systems sparse set based ECS systems are trivial to    implement correctly and understand, for nearly equivalent performance anyway, and
2. crucially, **archetype-based ECS are much slower when adding or removing    components from entities**, since when the signature of an entity changes,    that entire entity including all of its components must be removed from one    archetype pool, new memory must be allocated in another archetype pool, and    then the entity must be inserted there. This in essence defeats the *entire    purpose* of ECS as a game object architecture with unique benefits, by making    the one truly unique thing ECSs can do that isn't just an under the hood    optimization *the most expensive possible operation*, more expensive even    than anything equivalent in non-ECS systems, since it requires essentially    copying your object and reallocating memory. So it is all well and good if    you are using entity component systems primarily as a data acceleration    structure, in order to enable better data oriented design, but it is less    good if you want to use entity components because of the benefits specific to    ECS, such as being able to dynamically change the properties and behavior of    objects at runtime in a composable and encapsulated manner, or the ability to    use your ECS as a sort of intelligent database and query system over your    game world that can be used for dynamic and ephemeral properties using things    like tag components. So you see, while archetype-based ECS optimizes for the    general case, in doing so it actually undercuts much of the unique benefits    of ECS in the first place, reducing it to just an acceleration data    structure.

Note: what I'm saying about the relative performance characteristics of sparse set and archetype-based ECS systems isn't just based on a theoretical understanding of the nature of their algorithms either. It bears itself out in objective testing via benchmarks as well: <https://github.com/abeimler/ecs_benchmark>

Another case study would be my choice of spatial partitioning algorithms for location and distance queries, broad phase collision detection, and frustum culling, which are all important for large worlds and performant intelligent AI. The new hotness right now according to many people and several textbooks that I have read on the subject is bounding volume hierarchies, because due to the fact that they are built from the objects themselves instead of subdividing the space the objects are in and then sorting the objects into that space, they can require less work and do more accurate narrowing since the subdivisions fit more tightly around specific objects, so you don't for instance get fairly widely separated objects in the same node just because they're within a certain radius, and you don't get small objects way too high in your partitioning hierarchy just because they happen to fall on a bunch of spatial partition boundaries. So they are to some degree faster in a general sense. the issue is that they are in fact less suitable for situations where you have a lot of dynamically moving objects, since the hierarchy is itself built from the objects and so will likely need to be completely rebuilt anytime an objects moves or risk degrading over time until its performance is terrible if you use an insertion method. Despite this, in recent years people have begun recommending then even for dynamic objects for some reason. On the other hand, traditional spatial partitioning is much better than bounding volume hierarchies for dynamically moving objects, because since it is space itself that you are partitioning, when an object moves you rarely need to totally rebuild the hierarchy. At most you will need to split a node here or there into multiple leaf nodes, and in the majority of cases you will just remove the moving object from one node and put it in another. So, since I'm frankly not too concerned with having a best case broad phase algorithm for collisions with static meshes, since in any given world chunk there will probably only be a few large static meshes on the scene, and any complexities to their geometry have will have to be dealt with in the narrow phase for collision resolution anyway, which isn't even going to be handled by my engine but instead by a physics middleware, a slightly non-optimal solution for static meshes is probably acceptable. Meanwhile, I *will* have a large amount of dynamically moving entities in most world chunks, so I instead chose to go with spatial partitioning. The traditional spatial partitioning algorithm for 3D spaces is the octree, but given the fact that 90% of the kind of games that I want my engine to enable will take place in a setting that has some form or degree of gravity and is more horizontally oriented than vertically oriented, I decided that even those were overkill and went for a quadtree.

A related choice I made is in the fundamental architecture of my engine: it has a separate render and update thread that are not synchronized whatsoever. The render thread works on rendering whatever the last game state it has is, as fast as the sync rate or GPU performance lets it, while the update thread runs as fast as the update interval will allow it to, sending new states that need to be rendered to the render thread whenever they are available via a channel, which then get queued to be rendered as soon as possible. This is similar to how the Destiny rendering engine communicates with its update threads.[^11]

My engine goes even further than this in the direction of the Destiny engine, however. On the update thread parallelism is employed even more thoroughly: thanks to rayon, my game has a pool of threads equal in size to the number of cores on the CPU, not including the render an update thread, and so almost every task on the update thread is done by creating jobs on the job queue for the thread pool to take care of, whether that is loading and processing resources, or handling each iteration of various systems, or even running multiple systems at the same time if their data requirements allow that, maximizing the throughput of the system as best I can, to allow for game logic to be as slow and complicated as it needs to be. In essence, I want to give the user headroom. And thanks to the fact that the language used for game logic will be sandboxed and probably garbage collected, or at least reference counted, due to running in WASIX, and all mutations made in scripts being transparently transformed into command lists for lockless thread safety, this multithreading can be fully and fearlessly taken advantage of by the game programmer.

> [!NOTE]
> The core set of game systems in Embryo is much more rigid than, say, Bevy: you can't specify your own low level Rust systems, only your own scripted systems.

That's all pretty par for the course for Rust ECS systems though --- my more interesting choice was to actually pipeline all scripted systems. The way other Rust ECS systems do their parallel processing still employs the scatter-gather methodology, where each sequential system farms out all of its work as a bunch of jobs on the thread pool and then joins on those jobs and blocks until they all finish, and only after that system is done blocking and all of those jobs have finished can work proceed to any of the systems that depend on that one. Some of them also attempt to parallelize some systems, so if you have two systems that access a disjoint set of components, they can run at the same time, but this is barely the synchronous function parallel model[^8], which is actually ironically what has been holding Unreal Engine back from efficiently using CPUs in the modern day.[^9] This still has some inherent problems.

Instead of doing this, I tried to bring in some of the insights that the Destiny[^10][^11] and Naughty Dog GDC talks provided, by instead transforming all of the scripted systems for entities in the game engine into per entity "threads" (in the long winding string sense) through the pipeline of those systems, where the thread only goes through the single iteration of each system relevant to the given entity.

Essentially, instead of it working on the basis of rows, it works on the basis of columns. This means that entities can proceed through the entire pipeline of systems relevant to them without having to wait for other entities to catch up at the boundary between each sequential system, so that each entity in the game can have radically different systems running on them that may not match in their place in the pipeline (for example, one entity may have two systems in between the next system that it has in common with another entity, thereby forcing the second entity to wait before it can process its next system behavior), or can have systems where each iteration may take a radically different amount of time, and things won't get slowed down.

> [!IMPORTANT]
> Basically this means that the performance of each entity's updates won't be held down by the total number of systems or game mechanics you have running, only by the ones that apply to them, hopefully making your life a little easier. Of course, everything still has to sync up at the end of the update frame, or else you get race conditions.

Of course this is a trade-off, because it gives up a lot of the data oriented design benefits of doing that processing using systems, which is why all of the built-in engine systems will actually use traditional scatter/gather batch processing methodology, since the execution time of each individual iteration is unlikely to vary too much or to be particularly long and there are unlikely to be very long chains of built-in systems that entities have to go through that are different between different entities, so having to wait for each other to catch up at the boundary of each dependent sequential system is unlikely to be a problem.

But for scripted systems running in a WebAssembly runtime, which are likely to be small and numerous and highly variable between individual entities, and are likely to also be much slower than Rust code and more variable in their execution time, I think this was a trade-off that was crucial to make. Importantly, the reason I spent so much time on parallel processing is that modern CPUs are much more powerful in terms of parallel processing then they are in terms of single core straight line speed, and that divergence is likely only to increase, so optimizing for parallel processing is a core and fundamental way to give myself Headroom with single course sequential processing operations, so I don't have to be cycle counting as much.

At the same time, I used an approach similar to Naughty Dog's, where almost all tasks are done as jobs on thread pools, but sequential tasks are made sequential by being sequentially launched and joined in other threads, and *didn't* attempt a full directed acyclic graph of individual tasks and dependencies so that I could run the entire engine as jobs, like Destiny did, because that wasn't necessary for my use case.

> [!CAUTION]
> TLDR: this column-based approach is something that most ECSs don't try, usually because the performance benefits of branchless, cache-local execution of systems outweigh the parallelism benefits of this, but in my case, since most systems will probably be scripted and running in a WASIX runtime anyway, and might be JIT'd too, my cache-locality and branchlessness are shot to hell anyway, so I have to make up for it somehow!

## A unique take on Rust game development

As someone who is a huge proponent of Rust and actively using it within the game development ecosystem, you might be surprised to hear this, but I think the existing Rust game development landscape[^13] is **fundamentally misguided and doing it wrong**.

Rust is the next-gen replacement for C++, in my opinion --- of best use in embedded and systems programming contexts, and maybe application or CLI development if you're brave or have specific needs. But instead of trying to follow C++'s lead in the game development industry --- as the language of primarily game engines and very low level tight game loops --- the people pushing forward the Rust game development ecosystem seem to be trying to replace C# instead, forcing the use of Rust for high level game logic and gameplay programming through library design.

We can see this with the Rust bindings for scripting Godot, where Rust should actually probably be integrated into the Godot engine, not used as a scripting language for developing *with* the engine, and we can see it even more strongly in the current most hyped Rust game engines, Bevy and Amathyst, too: despite their billing, they are actually more akin to game *frameworks* --- that is, large callable collections of code that you attach to your game's own code by including the framework like you would a library and providing the framework with callbacks to your code, and then compiling your code along with the entire game framework into a single executable --- which means that you're forced to write your game logic in Rust, not just your engine.

In being structured this way, while marketing themselves as the premier Rust game engines, I believe these projects are actively hurting the entire Rust game development ecosystem: the problem with this is that although Rust is an excellent language, Rust is simply not very well-suited to the demands of game development at all[^12][^14]. This means that despite these libraries are sucking up the vast majority of attention, hype, recommendations, contributions, funding, and activity, they aren't actually used very much, and are probably not going to become very widely used or successful in the future, because experienced game developers will take one look at this game "engine," or maybe try it out with a few games, and quickly realize it isn't all it's cracked up to be, and that Rust isn't suited to this, leave a negative comment or two, and leave the ecosystem for good, all the while these same game frameworks continue taking up all the community's resources and focus, ensuring that nothing better comes along, because nobody in the community seems to have realized that these engines aren't going to work yet.

My aim is to do away with this. While my engine may not have a proper editor for a long time, that doesn't change the fact that there needs to be a clear separation between gameplay, game logic, game assets, and the engine itself, for it to be feasible using Rust. I intend to assert this distinction as strenuously as possible, thereby hopefully presenting a workable model of Rust game engine design.

[^1]: See this talk to understand what data-driven means for engines:     <https://gdcvault.com/play/1022543/A-Data-Driven-Object>.

[^2]: This talk is an especially good summary of how data-driven design, a     flexible entity-component system, and a message-passing based event system     that bubbles events up through the components on each entity much like     events bubble up through DOM elements, is an especially good one for     understanding what I'm talking about here:     <https://www.youtube.com/watch?v=U03XXzcThGU>

[^3]: https://en.wikipedia.org/wiki/Actor_model

[^4]: https://www.gdcvault.com/play/1022186/Parallelizing-the-Naughty-Dog-Engine

[^6]: https://en.m.wikipedia.org/wiki/Data-oriented_design

[^7]: For a basic intro about what these are, see the excellent *ECS back and     forth* series, part 2: https://skypjack.github.io/2019-03-07-ecs-baf-part-2/

[^8]:     <https://www.gamedeveloper.com/programming/multithreaded-game-engine-architectures>.     See also: <https://vkguide.dev/docs/extra-chapter/multithreading/>

[^9]: https://www.youtube.com/watch?v=-x0orqFrHGA

[^10]: https://www.youtube.com/watch?v=v2Q_zHG3vqg

[^11]: https://www.gdcvault.com/play/1021926/Destiny-s-Multithreaded-Rendering

[^12]: This article outlines the problems very eloquently and in a very informed manner: <https://loglog.games/blog/leaving-rust-gamedev/>. I would quibble with some of the points made --- I think generational arenas, ECS, and command lists are all incredibly good and beneficial patterns that almost entirely resolve the problems they're meant to deal with, and the problems he has with them really aren't that serious, so I don't think the fact that Rust forces you into them is that bad --- but I think the overall point is solid. Rust is a language designed around statically verifiable, or at the *very least* asserted at runtime, safety and correctness, as well as maximally predictable, clean, maintainable, well-architected code, explicitly and admittedly at the cost of flexibility, dynamicism, concision, ease of modification, and "just letting you do shit." It's closer to the Idris or Ada end of the spectrum than the Ruby or Common Lisp end. That's good when we want to write operating systems, large applications, compositors, crucial server components for e.g. parsing data formats, or whatever, but it runs directly against the grain of what is needed in game development: games typically have a pretty short development cycle --- they'll only be actively worked on for maybe a few years at most --- and additionally a pretty short life cycle --- users may play them for years or decades, but each run of the game probably won't be more than a few hours long, and state is regularly saved, so a crash here or there isn't an issue --- and games typically aren't security critical unless they're online games with microtransactions, so you don't really care too much about the statically verifiable correctness, safety, stability, and maintainability of your code. Meanwhile, what you do have a lot of in game development is quick prototyping, fast iteration, experimentation, complex systems that need to be able to influence each other from across the program, quickly evolving and changing requirements that are often never fully explained, and more, things Rust is extremely bad at. Not to mention Rust's compile times, and the fact that if you're using a game "engine" (framework) like Bevy, you have to compile the entire game "engine" and statically link it into your compiled game code, essentially completely ruling out hot reloading and modding and forcing you to compile the entire engine just to test your code. This makes Rust a very bad choice for developing individual games/game logic. On the other hand, game *engines* often are code bases that are worked on for years or even decades by huge teams, and, as a platform for games (the actually performance-heavy, buggy code) to be written on, and an application that will probably be used extremely and for long periods of time with much more difficult to save state, need to provide as solid, bug-free, and very performant experience to be useful, so Rust makes sense for game engines.
[^13]: https://arewegameyet.rs/
[^14]: and many experienced game developers who have tried Rust, or experienced Rust developers, seem to agree with this article, too, so it isn't just me and this one guy: <https://news.ycombinator.com/item?id=40172033>
[^15]: almost all of it undeserved in my opinion
